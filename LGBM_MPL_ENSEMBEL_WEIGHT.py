import os
import numpy as np
import pandas as pd
import lightgbm as lgb
import tensorflow as tf
from tensorflow.keras.models import Sequential
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import accuracy_score, log_loss, f1_score, recall_score, precision_score
from google.colab import drive
import joblib  # LightGBM Î™®Îç∏ Ï†ÄÏû•ÏùÑ ÏúÑÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨

# ‚úÖ Google Drive ÎßàÏö¥Ìä∏
drive.mount('/content/drive/')

# ‚úÖ Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú ÏÑ§Ï†ï
BASE_PATH = "/content/drive/MyDrive/hepscope/train_val_sample_test/deg.all.fc0.5/"
RESULTS_PATH = "/content/drive/MyDrive/hepscope/results/LGBM_MPL_ENSEMBEL_WEIGHT/"
MERGED_RESULTS_PATH = os.path.join(RESULTS_PATH, "merged_lbgm_mlp_ensemble_weight_results_external.csv")
os.makedirs(RESULTS_PATH, exist_ok=True)  # Í≤∞Í≥º Ìè¥Îçî ÏÉùÏÑ±

# ‚úÖ ÌååÏùº Î°úÎìú Ìï®Ïàò
def load_csv(file_path):
    try:
        return pd.read_csv(file_path).drop(columns=['Unnamed: 0'], errors='ignore')
    except Exception as e:
        print(f"‚ùå ÌååÏùº Î°úÎìú Ïã§Ìå® ({file_path}): {e}")
        return None

# ‚úÖ Îç∞Ïù¥ÌÑ∞ Ï†ïÍ∑úÌôî Î∞è Í≤∞Ìï© Ìï®Ïàò
def prepare_data(normal_df, malig_df, scaler=None):
    if normal_df is None or malig_df is None:
        return None, None, None
    normal_df['label'] = 0
    malig_df['label'] = 1
    data = pd.concat([normal_df, malig_df]).reset_index(drop=True)
    y = data.pop('label').values
    if scaler:
        x = scaler.transform(data)
    else:
        scaler = RobustScaler()
        x = scaler.fit_transform(data)
    return x, y, scaler

# ‚úÖ Í∞úÎ≥Ñ train_case ÌïôÏäµ Î∞è ÌèâÍ∞Ä Ìï®Ïàò
def process_train_case(train_case):
    case_path = os.path.join(BASE_PATH, train_case)
    result_case_path = os.path.join(RESULTS_PATH, train_case)  # train_caseÎ≥Ñ Í≤∞Í≥º Ìè¥Îçî ÏÉùÏÑ±
    os.makedirs(result_case_path, exist_ok=True)

    print(f"\nüîπ Processing {train_case} ...")

    # üîπ Îç∞Ïù¥ÌÑ∞ Î°úÎìú
    train_normal = load_csv(os.path.join(case_path, 'train_Normal.csv'))
    train_malig = load_csv(os.path.join(case_path, 'train_Malig.csv'))

    if train_normal is None or train_malig is None:
        print(f"‚ùå {train_case} Îç∞Ïù¥ÌÑ∞ Î∂ÄÏ°±ÏúºÎ°ú ÌïôÏäµ Ïä§ÌÇµ")
        return None

    # üîπ Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨
    train_x, train_y, scaler = prepare_data(train_normal, train_malig)

    results = []

    # ‚úÖ External ValidationÏùÑ Î™®Îì† val_ ÎîîÎ†âÌÜ†Î¶¨Ïóê ÎåÄÌï¥ ÏàòÌñâ
    val_dirs = [d for d in os.listdir(case_path) if d.startswith("val_")]
    for val_dir in val_dirs:
        val_path = os.path.join(case_path, val_dir)
        val_normal = load_csv(os.path.join(val_path, 'val_external_Normal.csv'))
        val_malig = load_csv(os.path.join(val_path, 'val_external_Malig.csv'))

        if val_normal is None or val_malig is None:
            print(f"‚ùå {train_case} - {val_dir} Îç∞Ïù¥ÌÑ∞ Î∂ÄÏ°±ÏúºÎ°ú Ïä§ÌÇµ")
            continue

        val_x, val_y, _ = prepare_data(val_normal, val_malig, scaler)

        # ‚úÖ LGBM ÌïôÏäµ
        lgbm_model = lgb.LGBMClassifier(
            objective="binary",
            metric="binary_logloss",
            n_estimators=200,
            learning_rate=0.05,
            max_depth=6,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_lambda=0.1,
            random_state=42
        )
        lgbm_model.fit(train_x, train_y)

        # ‚úÖ MLP ÌïôÏäµ
        mlp_model = Sequential([
            tf.keras.layers.Dense(256, activation='relu', input_shape=(train_x.shape[1],)),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.Dense(64, activation='relu'),
            tf.keras.layers.Dense(1, activation='sigmoid')
        ])
        mlp_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])
        mlp_model.fit(train_x, train_y, epochs=50, batch_size=32, validation_split=0.2, verbose=1)

        # ‚úÖ Î™®Îç∏ Ï†ÄÏû•
        lgbm_model_path = os.path.join(result_case_path, f"{train_case}_LGBM.txt")
        mlp_model_path = os.path.join(result_case_path, f"{train_case}_MLP.h5")
        joblib.dump(lgbm_model, lgbm_model_path)  # LightGBM Î™®Îç∏ Ï†ÄÏû•
        mlp_model.save(mlp_model_path)  # MLP Î™®Îç∏ Ï†ÄÏû•
        print(f"‚úÖ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: {lgbm_model_path}, {mlp_model_path}")

        # ‚úÖ External Validation ÌèâÍ∞Ä
        val_lgbm_preds = lgbm_model.predict_proba(val_x)[:, 1]
        val_mlp_preds = mlp_model.predict(val_x).flatten()

        # ‚úÖ Train & Validation Loss Í≥ÑÏÇ∞
        train_lgbm_loss = log_loss(train_y, lgbm_model.predict_proba(train_x)[:, 1])
        train_mlp_loss = log_loss(train_y, mlp_model.predict(train_x).flatten())
        val_lgbm_loss = log_loss(val_y, val_lgbm_preds)
        val_mlp_loss = log_loss(val_y, val_mlp_preds)

        # ‚úÖ Log Loss Í∏∞Î∞ò Í∞ÄÏ§ëÏπò Í≥ÑÏÇ∞
        epsilon = 1e-6
        weight_lgbm = 1 / (val_lgbm_loss + epsilon)
        weight_mlp = 1 / (val_mlp_loss + epsilon)

        # ‚úÖ Í∞ÄÏ§ë ÌèâÍ∑† ÏòàÏ∏°
        weight_sum = weight_lgbm + weight_mlp
        weight_lgbm /= weight_sum
        weight_mlp /= weight_sum

        ensemble_preds = (weight_lgbm * val_lgbm_preds + weight_mlp * val_mlp_preds)
        ensemble_pred_binary = (ensemble_preds > 0.5).astype(int)

        f1 = f1_score(val_y, ensemble_pred_binary)
        recall = recall_score(val_y, ensemble_pred_binary)
        precision = precision_score(val_y, ensemble_pred_binary)
        accuracy = accuracy_score(val_y, ensemble_pred_binary)

        # ‚úÖ Generalization Gap & Loss Ratio
        generalization_gap_lgbm = accuracy_score(train_y, (lgbm_model.predict_proba(train_x)[:, 1] > 0.5).astype(int)) - accuracy
        generalization_gap_mlp = accuracy_score(train_y, (mlp_model.predict(train_x).flatten() > 0.5).astype(int)) - accuracy
        loss_ratio_lgbm = val_lgbm_loss / (train_lgbm_loss + epsilon)
        loss_ratio_mlp = val_mlp_loss / (train_mlp_loss + epsilon)

        results.append([train_case, val_dir, f1, recall, precision, accuracy, weight_lgbm, weight_mlp, generalization_gap_lgbm, generalization_gap_mlp, loss_ratio_lgbm, loss_ratio_mlp])
        
    # ‚úÖ train_caseÎ≥Ñ External Validation Í≤∞Í≥º Ï†ÄÏû•
    results_df = pd.DataFrame(results, columns=[
        "Train Case", "Dataset", "F1 Score", "Recall", "Precision", "Accuracy",
        "LGBM Weight", "MLP Weight",
        "LGBM Generalization Gap", "MLP Generalization Gap",
        "LGBM Loss Ratio", "MLP Loss Ratio"
    ])
    results_df.to_csv(os.path.join(result_case_path, f"{train_case}_LGBM_MLP_Ensemble_WEIGHT_Results_external.csv"), index=False)
    return results_df

# ‚úÖ Ïã§Ìñâ Î∞è Í≤∞Í≥º Î≥ëÌï©
all_results = [process_train_case(case) for case in sorted(os.listdir(BASE_PATH)) if case.startswith("train_case_")]
if all_results:
    pd.concat(all_results).to_csv(MERGED_RESULTS_PATH, index=False)
    print(f"\n‚úÖ Í≤∞Í≥º Î≥ëÌï© ÏôÑÎ£å: {MERGED_RESULTS_PATH}")
